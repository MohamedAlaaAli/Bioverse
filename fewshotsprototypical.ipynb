{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Few Shot Image Classification With Prototipical Networks"]},{"cell_type":"markdown","metadata":{},"source":["#### Importing Modules"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-30T20:40:34.541979Z","iopub.status.busy":"2023-08-30T20:40:34.541606Z","iopub.status.idle":"2023-08-30T20:40:49.113777Z","shell.execute_reply":"2023-08-30T20:40:49.112696Z","shell.execute_reply.started":"2023-08-30T20:40:34.541939Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: easyfsl in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n","Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easyfsl) (3.7.1)\n","Requirement already satisfied: pandas>=1.4.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easyfsl) (2.0.2)\n","Requirement already satisfied: torch>=1.4.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easyfsl) (2.0.0+cu117)\n","Requirement already satisfied: torchvision>=0.7.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easyfsl) (0.15.1+cu117)\n","Requirement already satisfied: tqdm>=4.1.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easyfsl) (4.65.0)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (4.39.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.24.1)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.4.0->easyfsl) (2021.3)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.4.0->easyfsl) (2023.3)\n","Requirement already satisfied: filelock in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->easyfsl) (3.9.0)\n","Requirement already satisfied: typing-extensions in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->easyfsl) (4.4.0)\n","Requirement already satisfied: sympy in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->easyfsl) (1.11.1)\n","Requirement already satisfied: networkx in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->easyfsl) (3.0)\n","Requirement already satisfied: jinja2 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->easyfsl) (3.0.3)\n","Requirement already satisfied: requests in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision>=0.7.0->easyfsl) (2.28.2)\n","Requirement already satisfied: colorama in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.1.0->easyfsl) (0.4.4)\n","Requirement already satisfied: six>=1.5 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->easyfsl) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.4.0->easyfsl) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (3.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\egypt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.4.0->easyfsl) (1.2.1)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (c:\\users\\egypt\\appdata\\roaming\\python\\python310\\site-packages)\n"]}],"source":["!pip install easyfsl\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import Omniglot\n","from torchvision.models import resnet18\n","from tqdm import tqdm\n","from easyfsl.samplers import TaskSampler\n","from easyfsl.utils import plot_images, sliding_average"]},{"cell_type":"markdown","metadata":{},"source":["#### Data Loading and preprocessing "]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-30T20:40:49.116662Z","iopub.status.busy":"2023-08-30T20:40:49.116046Z","iopub.status.idle":"2023-08-30T20:40:49.122302Z","shell.execute_reply":"2023-08-30T20:40:49.120681Z","shell.execute_reply.started":"2023-08-30T20:40:49.116628Z"},"trusted":true},"outputs":[],"source":["image_size = 128\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","from torchvision.datasets import ImageFolder"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:40:49.124523Z","iopub.status.busy":"2023-08-30T20:40:49.123887Z","iopub.status.idle":"2023-08-30T20:40:49.257424Z","shell.execute_reply":"2023-08-30T20:40:49.256259Z","shell.execute_reply.started":"2023-08-30T20:40:49.124489Z"},"trusted":true},"outputs":[],"source":["train_transform = transforms.Compose(\n","        [\n","            transforms.Grayscale(num_output_channels=3),\n","            transforms.RandomResizedCrop(image_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","        ])\n","\n","train_set = ImageFolder(root=\"train\", transform=train_transform)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#r = next(iter(train_set))[0]\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 376\n","    Root location: train\n","    StandardTransform\n","Transform: Compose(\n","               Grayscale(num_output_channels=3)\n","               RandomResizedCrop(size=(128, 128), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n","               RandomHorizontalFlip(p=0.5)\n","               ToTensor()\n","           )"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_set"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:40:49.261599Z","iopub.status.busy":"2023-08-30T20:40:49.261265Z","iopub.status.idle":"2023-08-30T20:40:49.395180Z","shell.execute_reply":"2023-08-30T20:40:49.394201Z","shell.execute_reply.started":"2023-08-30T20:40:49.261571Z"},"trusted":true},"outputs":[],"source":["test_transform = transforms.Compose(\n","        [\n","            # Omniglot images have 1 channel, but our model will expect 3-channel images\n","            transforms.Grayscale(num_output_channels=3),\n","            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n","            transforms.CenterCrop(image_size),\n","            transforms.ToTensor(),\n","        ])\n","\n","test_set = ImageFolder(root=\"/kaggle/input/cats-and-dogs-small/test\", transform=train_transform)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Modeling"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:40:49.397439Z","iopub.status.busy":"2023-08-30T20:40:49.396743Z","iopub.status.idle":"2023-08-30T20:40:49.402705Z","shell.execute_reply":"2023-08-30T20:40:49.401633Z","shell.execute_reply.started":"2023-08-30T20:40:49.397393Z"},"trusted":true},"outputs":[],"source":["N_WAY = 2  # Number of classes in a task\n","N_SHOT = 5  # Number of images per class in the support set\n","N_QUERY = 10  # Number of images per class in the query set\n","N_EVALUATION_TASKS = 100"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-30T20:40:49.405500Z","iopub.status.busy":"2023-08-30T20:40:49.404692Z","iopub.status.idle":"2023-08-30T20:40:53.553709Z","shell.execute_reply":"2023-08-30T20:40:53.552701Z","shell.execute_reply.started":"2023-08-30T20:40:49.405463Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 61.7MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Flatten(start_dim=1, end_dim=-1)\n",")\n"]}],"source":["class PrototypicalNetworks(nn.Module):\n","    def __init__(self, backbone: nn.Module):\n","        super(PrototypicalNetworks, self).__init__()\n","        self.backbone = backbone\n","\n","    def forward(\n","        self,\n","        support_images: torch.Tensor,\n","        support_labels: torch.Tensor,\n","        query_images: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Predict query labels using labeled support images.\n","        \"\"\"\n","        # Extract the features of support and query images\n","        z_support = self.backbone.forward(support_images)\n","        z_query = self.backbone.forward(query_images)\n","\n","        # Infer the number of different classes from the labels of the support set\n","        n_way = len(torch.unique(support_labels))\n","        # Prototype i is the mean of all instances of features corresponding to labels == i\n","        z_proto = torch.cat(\n","            [\n","                z_support[torch.nonzero(support_labels == label)].mean(0)\n","                for label in range(n_way)\n","            ]\n","        )\n","\n","        # Compute the euclidean distance from queries to prototypes\n","        dists = torch.cdist(z_query, z_proto)\n","\n","        # And here is the super complicated operation to transform those distances into classification scores!\n","        scores = -dists\n","        return scores\n","\n","\n","convolutional_network = resnet18(pretrained=True) # Load a pretrained resnet18 backbone\n","convolutional_network.fc = nn.Flatten()\n","print(convolutional_network)\n","\n","model = PrototypicalNetworks(convolutional_network).cuda()"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-30T20:40:53.555593Z","iopub.status.busy":"2023-08-30T20:40:53.555231Z","iopub.status.idle":"2023-08-30T20:40:53.566485Z","shell.execute_reply":"2023-08-30T20:40:53.565521Z","shell.execute_reply.started":"2023-08-30T20:40:53.555559Z"},"trusted":true},"outputs":[],"source":["def evaluate_on_one_task(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n","    query_labels: torch.Tensor,\n",") -> [int, int]:\n","    \"\"\"\n","    Returns the number of correct predictions of query labels, and the total number of predictions.\n","    \"\"\"\n","    return (\n","        torch.max(\n","            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n","            .detach()\n","            .data,\n","            1,\n","        )[1]\n","        == query_labels.cuda()\n","    ).sum().item(), len(query_labels)\n","\n","\n","def evaluate(data_loader: DataLoader):\n","    # We'll count everything and compute the ratio at the end\n","    total_predictions = 0\n","    correct_predictions = 0\n","\n","    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n","    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n","    model.eval()\n","    with torch.no_grad():\n","        for episode_index, (\n","            support_images,\n","            support_labels,\n","            query_images,\n","            query_labels,\n","            class_ids,\n","        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n","\n","            correct, total = evaluate_on_one_task(\n","                support_images, support_labels, query_images, query_labels\n","            )\n","\n","            total_predictions += total\n","            correct_predictions += correct\n","\n","    print(\n","        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["#### Training the model and evaluating "]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-30T20:40:53.568450Z","iopub.status.busy":"2023-08-30T20:40:53.568090Z","iopub.status.idle":"2023-08-30T20:47:39.650966Z","shell.execute_reply":"2023-08-30T20:47:39.649805Z","shell.execute_reply.started":"2023-08-30T20:40:53.568419Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","100%|██████████| 4000/4000 [06:37<00:00, 10.07it/s, loss=0.215] \n","100%|██████████| 100/100 [00:08<00:00, 11.47it/s]"]},{"name":"stdout","output_type":"stream","text":["Model tested on 100 tasks. Accuracy: 80.65%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["N_TRAINING_EPISODES = 4000\n","N_VALIDATION_TASKS = 100\n","\n","train_set.get_labels = lambda: [instance for instance in train_set.targets]\n","train_sampler = TaskSampler(\n","    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",")\n","train_loader = DataLoader(\n","    train_set,\n","    batch_sampler=train_sampler,\n","    num_workers=4,\n","    pin_memory=True,\n","    collate_fn=train_sampler.episodic_collate_fn,\n",")\n","\n","test_set.get_labels = lambda: [\n","    instance for instance in test_set.targets\n","]\n","test_sampler = TaskSampler(\n","    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",")\n","\n","test_loader = DataLoader(\n","    test_set,\n","    batch_sampler=test_sampler,\n","    num_workers=4,\n","    pin_memory=True,\n","    collate_fn=test_sampler.episodic_collate_fn,\n",")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","def fit(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n","    query_labels: torch.Tensor,\n",") -> float:\n","    optimizer.zero_grad()\n","    classification_scores = model(\n","        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n","    )\n","\n","    loss = criterion(classification_scores, query_labels.cuda())\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()\n","\n","# Train the model yourself with this cell\n","\n","log_update_frequency = 10\n","\n","all_loss = []\n","model.train()\n","with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n","    for episode_index, (\n","        support_images,\n","        support_labels,\n","        query_images,\n","        query_labels,\n","        _,\n","    ) in tqdm_train:\n","        loss_value = fit(support_images, support_labels, query_images, query_labels)\n","        all_loss.append(loss_value)\n","\n","        if episode_index % log_update_frequency == 0:\n","            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n","\n","evaluate(test_loader)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:48:30.898461Z","iopub.status.busy":"2023-08-30T20:48:30.897384Z","iopub.status.idle":"2023-08-30T20:48:31.033793Z","shell.execute_reply":"2023-08-30T20:48:31.032825Z","shell.execute_reply.started":"2023-08-30T20:48:30.898384Z"},"trusted":true},"outputs":[],"source":["save_path = '/kaggle/working/model.pth'\n","\n","# Save the model's state dictionary to the specified file\n","torch.save(model, save_path)"]},{"cell_type":"markdown","metadata":{},"source":["#### Testing"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:53:20.029903Z","iopub.status.busy":"2023-08-30T20:53:20.029515Z","iopub.status.idle":"2023-08-30T20:53:20.923299Z","shell.execute_reply":"2023-08-30T20:53:20.921916Z","shell.execute_reply.started":"2023-08-30T20:53:20.029869Z"},"trusted":true},"outputs":[],"source":["(\n","    example_support_images,\n","    example_support_labels,\n","    example_query_images,\n","    example_query_labels,\n","    example_class_ids,\n",") = next(iter(test_loader))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:57:32.050071Z","iopub.status.busy":"2023-08-30T20:57:32.049637Z","iopub.status.idle":"2023-08-30T20:57:32.075131Z","shell.execute_reply":"2023-08-30T20:57:32.074037Z","shell.execute_reply.started":"2023-08-30T20:57:32.050038Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[-0.8218, -6.1963]], device='cuda:0')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["input_image = Image.open('/kaggle/input/cats-and-dogs-small/test/cats/cat.1523.jpg')  # Load your input image\n","preprocess = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize([image_size, image_size]),\n","    transforms.ToTensor(),\n","])\n","input_tensor = preprocess(input_image).unsqueeze(0)  # Add batch dimension\n","\n","# Perform inference\n","model.eval()\n","with torch.no_grad():\n","    scores = model(example_support_images.cuda(), example_support_labels.cuda(), input_tensor.cuda())\n","scores"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T20:57:34.858347Z","iopub.status.busy":"2023-08-30T20:57:34.857946Z","iopub.status.idle":"2023-08-30T20:57:34.864121Z","shell.execute_reply":"2023-08-30T20:57:34.863173Z","shell.execute_reply.started":"2023-08-30T20:57:34.858318Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["class : 0\n"]}],"source":["print(f\"class : {torch.argmax(scores)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
